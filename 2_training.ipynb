{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c63bafe9",
   "metadata": {},
   "source": [
    "# Part 2: Model Training (All 8 Models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce40403",
   "metadata": {},
   "source": [
    "## Installation Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6017a00a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (2.8.0+cu128)\n",
      "Requirement already satisfied: torchvision in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (0.23.0+cu128)\n",
      "Requirement already satisfied: numpy in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (1.26.4)\n",
      "Requirement already satisfied: matplotlib in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (3.8.2)\n",
      "Requirement already satisfied: tqdm in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (4.67.1)\n",
      "Requirement already satisfied: scikit-learn in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (1.3.2)\n",
      "Requirement already satisfied: filelock in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (3.4.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torchvision) (11.3.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from scikit-learn) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from jinja2->torch) (3.0.3)\n"
     ]
    }
   ],
   "source": [
    "# Install PyTorch and other dependencies\n",
    "!pip install torch torchvision numpy matplotlib tqdm scikit-learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f20a2d6",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95dd7e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU Memory - Free: 42.0 GB / Total: 42.4 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.amp import autocast, GradScaler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import time\n",
    "import glob\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Check GPU memory\n",
    "if torch.cuda.is_available():\n",
    "    free_mem, total_mem = torch.cuda.mem_get_info()\n",
    "    print(f\"GPU Memory - Free: {free_mem/1e9:.1f} GB / Total: {total_mem/1e9:.1f} GB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b13e259",
   "metadata": {},
   "source": [
    "## Load Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f18fdb33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Wikipedia dataset...\n",
      "Wikipedia - Vocab size: 78683, Samples: 674773\n",
      "Loading Linux kernel dataset...\n",
      "Linux - Vocab size: 90882, Samples: 779603\n"
     ]
    }
   ],
   "source": [
    "# Load Wikipedia data\n",
    "print(\"Loading Wikipedia dataset...\")\n",
    "with open('wikipedia_processed.pkl', 'rb') as f:\n",
    "    wiki_data = pickle.load(f)\n",
    "\n",
    "wiki_X = wiki_data['X']\n",
    "wiki_y = wiki_data['y']\n",
    "wiki_vocab_size = wiki_data['vocab_size']\n",
    "wiki_word_to_idx = wiki_data['word_to_idx']\n",
    "wiki_idx_to_word = wiki_data['idx_to_word']\n",
    "\n",
    "print(f\"Wikipedia - Vocab size: {wiki_vocab_size}, Samples: {len(wiki_X)}\")\n",
    "\n",
    "# Load Linux data\n",
    "print(\"Loading Linux kernel dataset...\")\n",
    "with open('linux_processed.pkl', 'rb') as f:\n",
    "    linux_data = pickle.load(f)\n",
    "\n",
    "linux_X = linux_data['X']\n",
    "linux_y = linux_data['y']\n",
    "linux_vocab_size = linux_data['vocab_size']\n",
    "linux_word_to_idx = linux_data['word_to_idx']\n",
    "linux_idx_to_word = linux_data['idx_to_word']\n",
    "\n",
    "print(f\"Linux - Vocab size: {linux_vocab_size}, Samples: {len(linux_X)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf18c5c",
   "metadata": {},
   "source": [
    "## Checkpoint Verification Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6edefe19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CHECKPOINT VERIFICATION\n",
      "================================================================================\n",
      "\n",
      "Found 8 checkpoint files:\n",
      "\n",
      "Checkpoint                               Size (MB)    Status\n",
      "----------------------------------------------------------------------\n",
      "linux_32d-ReLU_best.pth                      1154.7  ‚úÖ OK (epoch 1)\n",
      "linux_32d-tanh_best.pth                      1154.7  ‚úÖ OK (epoch 2)\n",
      "linux_64d-ReLU_best.pth                      1191.6  ‚úÖ OK (epoch 1)\n",
      "linux_64d-tanh_best.pth                      1191.6  ‚úÖ OK (epoch 2)\n",
      "wiki_32d-ReLU_best.pth                       1000.0  ‚úÖ OK (epoch 1)\n",
      "wiki_32d-tanh_best.pth                       1000.0  ‚úÖ OK (epoch 1)\n",
      "wiki_64d-ReLU_best.pth                       1032.2  ‚úÖ OK (epoch 1)\n",
      "wiki_64d-tanh_best.pth                       1032.2  ‚úÖ OK (epoch 1)\n",
      "\n",
      "‚úÖ 8/8 models have valid checkpoints\n"
     ]
    }
   ],
   "source": [
    "def verify_checkpoints():\n",
    "    \"\"\"List and verify all model checkpoints\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"CHECKPOINT VERIFICATION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    checkpoints = sorted(glob.glob(\"*_best.pth\"))\n",
    "    \n",
    "    if not checkpoints:\n",
    "        print(\"No checkpoints found. All models need training.\")\n",
    "        return {}\n",
    "    \n",
    "    verified = {}\n",
    "    print(f\"\\nFound {len(checkpoints)} checkpoint files:\\n\")\n",
    "    print(f\"{'Checkpoint':<40} {'Size (MB)':<12} {'Status'}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for ckpt_path in checkpoints:\n",
    "        size_mb = os.path.getsize(ckpt_path) / 1e6\n",
    "        model_name = ckpt_path.replace('_best.pth', '')\n",
    "        \n",
    "        try:\n",
    "            # Try loading\n",
    "            ckpt = torch.load(ckpt_path, map_location='cpu')\n",
    "            required_keys = ['epoch', 'model_state_dict', 'val_loss', 'val_accuracy']\n",
    "            \n",
    "            if all(k in ckpt for k in required_keys):\n",
    "                status = f\"‚úÖ OK (epoch {ckpt['epoch']+1})\"\n",
    "                verified[model_name] = {\n",
    "                    'path': ckpt_path,\n",
    "                    'epoch': ckpt['epoch'],\n",
    "                    'val_loss': ckpt['val_loss'],\n",
    "                    'val_accuracy': ckpt['val_accuracy']\n",
    "                }\n",
    "            else:\n",
    "                status = \"‚ö†Ô∏è Missing keys\"\n",
    "        except Exception as e:\n",
    "            status = f\"‚ùå Corrupted: {str(e)[:30]}\"\n",
    "        \n",
    "        print(f\"{ckpt_path:<40} {size_mb:>10.1f}  {status}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ {len(verified)}/{8} models have valid checkpoints\")\n",
    "    return verified\n",
    "\n",
    "# Run verification\n",
    "verified_models = verify_checkpoints()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96ce607",
   "metadata": {},
   "source": [
    "## Load or Initialize Results Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24b2af9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Loaded 0 models from all_models_results.pkl\n"
     ]
    }
   ],
   "source": [
    "# Load previously trained models if they exist\n",
    "all_models = {}\n",
    "\n",
    "if os.path.exists('all_models_results.pkl'):\n",
    "    try:\n",
    "        with open('all_models_results.pkl', 'rb') as f:\n",
    "            all_models = pickle.load(f)\n",
    "        print(f\"\\n‚úÖ Loaded {len(all_models)} models from all_models_results.pkl\")\n",
    "        for name in all_models.keys():\n",
    "            print(f\"   - {name}\")\n",
    "    except (EOFError, pickle.UnpicklingError) as e:\n",
    "        print(f\"\\n‚ö†Ô∏è Corrupted pickle file found: {e}\")\n",
    "        print(\"Renaming to backup and starting fresh...\")\n",
    "        os.rename('all_models_results.pkl', f'all_models_results_corrupted_{int(time.time())}.pkl')\n",
    "        all_models = {}\n",
    "else:\n",
    "    print(\"\\nNo previous all_models_results.pkl found. Starting fresh.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cdc05b6-b0ed-4b8c-80a4-65523e535f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rebuilt 8 entries from existing checkpoints. Total in memory: 8\n",
      "Saved rebuilt all_models_results.pkl\n"
     ]
    }
   ],
   "source": [
    "# Rebuild all_models from existing checkpoints if the summary file was empty/missing\n",
    "import pickle, os, glob\n",
    "\n",
    "def model_present(name):\n",
    "    return os.path.exists(f\"{name}_best.pth\")\n",
    "\n",
    "rebuilt = {}\n",
    "\n",
    "# Wiki models\n",
    "with open('wikipedia_processed.pkl', 'rb') as f:\n",
    "    wiki_data = pickle.load(f)\n",
    "for name in [\"wiki_64d-ReLU\",\"wiki_32d-ReLU\",\"wiki_64d-tanh\",\"wiki_32d-tanh\"]:\n",
    "    if model_present(name):\n",
    "        rebuilt[name] = {\n",
    "            'model': None,\n",
    "            'final_val_loss': None,\n",
    "            'final_val_accuracy': None,\n",
    "            'config': {'name': name},\n",
    "            'vocab_size': wiki_data['vocab_size'],\n",
    "            'dataset_type': 'wikipedia'\n",
    "        }\n",
    "\n",
    "# Linux models\n",
    "with open('linux_processed.pkl', 'rb') as f:\n",
    "    linux_data = pickle.load(f)\n",
    "for name in [\"linux_64d-ReLU\",\"linux_32d-ReLU\",\"linux_64d-tanh\",\"linux_32d-tanh\"]:\n",
    "    if model_present(name):\n",
    "        rebuilt[name] = {\n",
    "            'model': None,\n",
    "            'final_val_loss': None,\n",
    "            'final_val_accuracy': None,\n",
    "            'config': {'name': name},\n",
    "            'vocab_size': linux_data['vocab_size'],\n",
    "            'dataset_type': 'linux'\n",
    "        }\n",
    "\n",
    "# Merge into all_models\n",
    "all_models.update(rebuilt)\n",
    "print(f\"Rebuilt {len(rebuilt)} entries from existing checkpoints. Total in memory: {len(all_models)}\")\n",
    "\n",
    "# Save immediately so future runs load correctly\n",
    "with open('all_models_results.pkl', 'wb') as f:\n",
    "    pickle.dump(all_models, f)\n",
    "print(\"Saved rebuilt all_models_results.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1054a513",
   "metadata": {},
   "source": [
    "## Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6fdd16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordPredictionDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for word prediction\"\"\"\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.LongTensor(X)\n",
    "        self.y = torch.LongTensor(y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf183da",
   "metadata": {},
   "source": [
    "## MLP Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "226b0f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mlp_model(vocab_size, embedding_dim, hidden_size, context_length, activation='relu'):\n",
    "    \"\"\"\n",
    "    Create MLP model for next-word prediction\n",
    "    \n",
    "    Args:\n",
    "        vocab_size: Size of vocabulary\n",
    "        embedding_dim: Dimension of word embeddings (32 or 64)\n",
    "        hidden_size: Number of neurons in hidden layer (1024)\n",
    "        context_length: Number of context words (5)\n",
    "        activation: 'relu' or 'tanh'\n",
    "    \"\"\"\n",
    "    \n",
    "    class MLPWordPredictor(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(MLPWordPredictor, self).__init__()\n",
    "            \n",
    "            # Embedding layer\n",
    "            self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "            \n",
    "            # Calculate input size to hidden layer\n",
    "            input_size = context_length * embedding_dim\n",
    "            \n",
    "            # Hidden layer (single layer as per requirement)\n",
    "            self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "            \n",
    "            # Activation function\n",
    "            if activation == 'relu':\n",
    "                self.activation = nn.ReLU()\n",
    "            elif activation == 'tanh':\n",
    "                self.activation = nn.Tanh()\n",
    "            else:\n",
    "                raise ValueError(\"Activation must be 'relu' or 'tanh'\")\n",
    "            \n",
    "            # Output layer\n",
    "            self.fc2 = nn.Linear(hidden_size, vocab_size)\n",
    "            \n",
    "        def forward(self, x):\n",
    "            # x shape: (batch_size, context_length)\n",
    "            embedded = self.embedding(x)  # (batch_size, context_length, embedding_dim)\n",
    "            \n",
    "            # Flatten embeddings\n",
    "            embedded = embedded.reshape(embedded.size(0), -1)  # (batch_size, context_length * embedding_dim)\n",
    "            \n",
    "            # Hidden layer\n",
    "            hidden = self.activation(self.fc1(embedded))\n",
    "            \n",
    "            # Output layer\n",
    "            output = self.fc2(hidden)\n",
    "            \n",
    "            return output\n",
    "        \n",
    "        def get_embeddings(self):\n",
    "            \"\"\"Return the embedding weights\"\"\"\n",
    "            return self.embedding.weight.detach().cpu().numpy()\n",
    "    \n",
    "    return MLPWordPredictor()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898a1f55",
   "metadata": {},
   "source": [
    "## Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7de01251",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, num_epochs, learning_rate, device, model_name):\n",
    "    \"\"\"\n",
    "    Train the MLP model with mixed precision for speed\n",
    "    \n",
    "    Returns:\n",
    "        model: Trained model\n",
    "        train_losses: List of training losses\n",
    "        val_losses: List of validation losses\n",
    "        val_accuracies: List of validation accuracies\n",
    "    \"\"\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Mixed precision scaler\n",
    "    scaler = GradScaler()\n",
    "    amp_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Training {model_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_batches = 0\n",
    "        \n",
    "        train_pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Train]', leave=False)\n",
    "        for batch_X, batch_y in train_pbar:\n",
    "            batch_X = batch_X.to(device, non_blocking=True)\n",
    "            batch_y = batch_y.to(device, non_blocking=True)\n",
    "            \n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            \n",
    "            # Forward pass with mixed precision\n",
    "            with autocast(device_type=\"cuda\", dtype=amp_dtype):\n",
    "                outputs = model(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "            \n",
    "            # Backward pass with scaling\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_batches += 1\n",
    "            train_pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "        \n",
    "        avg_train_loss = train_loss / train_batches\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        val_batches = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in val_loader:\n",
    "                batch_X = batch_X.to(device, non_blocking=True)\n",
    "                batch_y = batch_y.to(device, non_blocking=True)\n",
    "                \n",
    "                # Forward pass with mixed precision\n",
    "                with autocast(device_type=\"cuda\", dtype=amp_dtype):\n",
    "                    outputs = model(batch_X)\n",
    "                    loss = criterion(outputs, batch_y)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                val_batches += 1\n",
    "                \n",
    "                # Calculate accuracy\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                val_total += batch_y.size(0)\n",
    "                val_correct += (predicted == batch_y).sum().item()\n",
    "        \n",
    "        avg_val_loss = val_loss / val_batches\n",
    "        val_accuracy = 100 * val_correct / val_total\n",
    "        \n",
    "        val_losses.append(avg_val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        \n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}] ({epoch_time:.1f}s) - '\n",
    "              f'Train Loss: {avg_train_loss:.4f} | '\n",
    "              f'Val Loss: {avg_val_loss:.4f} | '\n",
    "              f'Val Acc: {val_accuracy:.2f}%')\n",
    "        \n",
    "        # Save best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': avg_val_loss,\n",
    "                'val_accuracy': val_accuracy\n",
    "            }, f'{model_name}_best.pth')\n",
    "    \n",
    "    print(f\"\\n‚úÖ Training complete! Best val loss: {best_val_loss:.4f}\")\n",
    "    return model, train_losses, val_losses, val_accuracies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c8109f",
   "metadata": {},
   "source": [
    "## Prediction and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "affaf71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample_predictions(model, dataset_X, idx_to_word, device, num_samples=5):\n",
    "    \"\"\"Generate sample predictions from the model\"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(num_samples):\n",
    "            idx = np.random.randint(0, len(dataset_X))\n",
    "            context = torch.LongTensor([dataset_X[idx]]).to(device)\n",
    "            \n",
    "            amp_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "            with autocast(device_type=\"cuda\", dtype=amp_dtype):\n",
    "                output = model(context)\n",
    "            _, predicted_idx = torch.max(output, 1)\n",
    "            \n",
    "            context_words = [idx_to_word[int(idx)] for idx in dataset_X[idx]]\n",
    "            predicted_word = idx_to_word[int(predicted_idx.item())]\n",
    "            \n",
    "            predictions.append({\n",
    "                'context': ' '.join(context_words),\n",
    "                'predicted': predicted_word\n",
    "            })\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "def plot_training_curves(train_losses, val_losses, val_accuracies, model_name):\n",
    "    \"\"\"Plot training and validation curves\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Loss plot\n",
    "    ax1.plot(train_losses, label='Train Loss', linewidth=2)\n",
    "    ax1.plot(val_losses, label='Val Loss', linewidth=2)\n",
    "    ax1.set_xlabel('Epoch', fontsize=12)\n",
    "    ax1.set_ylabel('Loss', fontsize=12)\n",
    "    ax1.set_title(f'{model_name} - Training vs Validation Loss', fontsize=14, fontweight='bold')\n",
    "    ax1.legend(fontsize=11)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Accuracy plot\n",
    "    ax2.plot(val_accuracies, label='Val Accuracy', color='green', linewidth=2)\n",
    "    ax2.set_xlabel('Epoch', fontsize=12)\n",
    "    ax2.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "    ax2.set_title(f'{model_name} - Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "    ax2.legend(fontsize=11)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{model_name}_training_curves.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"üìä Training curves saved as '{model_name}_training_curves.png'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3f8a37",
   "metadata": {},
   "source": [
    "## Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd15bedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training configuration:\n",
      "  Context length: 5\n",
      "  Hidden size: 1024\n",
      "  Batch size: 1024\n",
      "  Epochs: 100\n",
      "  Learning rate: 0.001\n",
      "  Num workers: 8\n",
      "  Mixed precision: ENABLED\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "CONTEXT_LENGTH = 5\n",
    "HIDDEN_SIZE = 1024\n",
    "BATCH_SIZE = 1024  # Optimized for speed\n",
    "NUM_EPOCHS = 100  # Reduced for time constraints\n",
    "LEARNING_RATE = 0.001\n",
    "VAL_SPLIT = 0.1\n",
    "NUM_WORKERS = 8  # Parallel data loading\n",
    "PIN_MEMORY = True  # Faster GPU transfer\n",
    "\n",
    "# Model configurations\n",
    "model_configs = [\n",
    "    {'embedding_dim': 64, 'activation': 'relu', 'name': '64d-ReLU'},\n",
    "    {'embedding_dim': 32, 'activation': 'relu', 'name': '32d-ReLU'},\n",
    "    {'embedding_dim': 64, 'activation': 'tanh', 'name': '64d-tanh'},\n",
    "    {'embedding_dim': 32, 'activation': 'tanh', 'name': '32d-tanh'},\n",
    "]\n",
    "\n",
    "print(f\"Training configuration:\")\n",
    "print(f\"  Context length: {CONTEXT_LENGTH}\")\n",
    "print(f\"  Hidden size: {HIDDEN_SIZE}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Num workers: {NUM_WORKERS}\")\n",
    "print(f\"  Mixed precision: ENABLED\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59abc3ac",
   "metadata": {},
   "source": [
    "## Wikipedia Models Training (with Skip Logic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6354c34e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TRAINING WIKIPEDIA MODELS\n",
      "================================================================================\n",
      "\n",
      "‚è≠Ô∏è  Skipping wiki_64d-ReLU - already trained!\n",
      "   Checkpoint: wiki_64d-ReLU_best.pth\n",
      "   Val Loss: 7.3232\n",
      "   Val Accuracy: 13.34%\n",
      "\n",
      "‚è≠Ô∏è  Skipping wiki_32d-ReLU - already trained!\n",
      "   Checkpoint: wiki_32d-ReLU_best.pth\n",
      "   Val Loss: 7.4249\n",
      "   Val Accuracy: 12.37%\n",
      "\n",
      "‚è≠Ô∏è  Skipping wiki_64d-tanh - already trained!\n",
      "   Checkpoint: wiki_64d-tanh_best.pth\n",
      "   Val Loss: 7.4166\n",
      "   Val Accuracy: 13.23%\n",
      "\n",
      "‚è≠Ô∏è  Skipping wiki_32d-tanh - already trained!\n",
      "   Checkpoint: wiki_32d-tanh_best.pth\n",
      "   Val Loss: 7.5366\n",
      "   Val Accuracy: 12.30%\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING WIKIPEDIA MODELS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create datasets\n",
    "wiki_dataset = WordPredictionDataset(wiki_X, wiki_y)\n",
    "wiki_train_size = int((1 - VAL_SPLIT) * len(wiki_dataset))\n",
    "wiki_val_size = len(wiki_dataset) - wiki_train_size\n",
    "wiki_train_dataset, wiki_val_dataset = random_split(wiki_dataset, [wiki_train_size, wiki_val_size])\n",
    "\n",
    "wiki_train_loader = DataLoader(wiki_train_dataset, batch_size=BATCH_SIZE, shuffle=True, \n",
    "                               num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "wiki_val_loader = DataLoader(wiki_val_dataset, batch_size=BATCH_SIZE, shuffle=False, \n",
    "                             num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "\n",
    "for config in model_configs:\n",
    "    model_name = f\"wiki_{config['name']}\"\n",
    "    \n",
    "    # Skip if already trained (checkpoint exists and is valid)\n",
    "    if model_name in verified_models:\n",
    "        print(f\"\\n‚è≠Ô∏è  Skipping {model_name} - already trained!\")\n",
    "        print(f\"   Checkpoint: {verified_models[model_name]['path']}\")\n",
    "        print(f\"   Val Loss: {verified_models[model_name]['val_loss']:.4f}\")\n",
    "        print(f\"   Val Accuracy: {verified_models[model_name]['val_accuracy']:.2f}%\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\nüöÄ Starting training for {model_name}...\")\n",
    "    \n",
    "    # Create model\n",
    "    model = create_mlp_model(\n",
    "        vocab_size=wiki_vocab_size,\n",
    "        embedding_dim=config['embedding_dim'],\n",
    "        hidden_size=HIDDEN_SIZE,\n",
    "        context_length=CONTEXT_LENGTH,\n",
    "        activation=config['activation']\n",
    "    ).to(device)\n",
    "    \n",
    "    print(f\"\\nüìä Model: {model_name}\")\n",
    "    print(f\"   Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    # Train model\n",
    "    model, train_losses, val_losses, val_accuracies = train_model(\n",
    "        model, wiki_train_loader, wiki_val_loader, NUM_EPOCHS, LEARNING_RATE, device, model_name\n",
    "    )\n",
    "    \n",
    "    # Plot training curves\n",
    "    plot_training_curves(train_losses, val_losses, val_accuracies, model_name)\n",
    "    \n",
    "    # Generate sample predictions\n",
    "    print(f\"\\nüéØ Sample Predictions for {model_name}:\")\n",
    "    predictions = generate_sample_predictions(model, wiki_X, wiki_idx_to_word, device, num_samples=5)\n",
    "    for i, pred in enumerate(predictions, 1):\n",
    "        print(f\"   {i}. Context: '{pred['context']}' ‚Üí Predicted: '{pred['predicted']}'\")\n",
    "    \n",
    "    # Save results\n",
    "    all_models[model_name] = {\n",
    "        'model': model,\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'val_accuracies': val_accuracies,\n",
    "        'config': config,\n",
    "        'final_val_loss': val_losses[-1],\n",
    "        'final_val_accuracy': val_accuracies[-1],\n",
    "        'vocab_size': wiki_vocab_size,\n",
    "        'word_to_idx': wiki_word_to_idx,\n",
    "        'idx_to_word': wiki_idx_to_word,\n",
    "        'dataset_type': 'wikipedia'\n",
    "    }\n",
    "    \n",
    "    # Incremental save after each model\n",
    "    with open('all_models_results.pkl', 'wb') as f:\n",
    "        pickle.dump(all_models, f)\n",
    "    print(f\"üíæ Saved checkpoint with {len(all_models)} models so far\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ {model_name} training complete!\")\n",
    "    print(f\"   Final Val Loss: {val_losses[-1]:.4f}\")\n",
    "    print(f\"   Final Val Accuracy: {val_accuracies[-1]:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05553345",
   "metadata": {},
   "source": [
    "## Linux Kernel Models Training (with Skip Logic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a88db8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TRAINING LINUX KERNEL MODELS\n",
      "================================================================================\n",
      "\n",
      "‚è≠Ô∏è  Skipping linux_64d-ReLU - already trained!\n",
      "   Checkpoint: linux_64d-ReLU_best.pth\n",
      "   Val Loss: 4.9966\n",
      "   Val Accuracy: 35.33%\n",
      "\n",
      "‚è≠Ô∏è  Skipping linux_32d-ReLU - already trained!\n",
      "   Checkpoint: linux_32d-ReLU_best.pth\n",
      "   Val Loss: 5.0820\n",
      "   Val Accuracy: 34.16%\n",
      "\n",
      "‚è≠Ô∏è  Skipping linux_64d-tanh - already trained!\n",
      "   Checkpoint: linux_64d-tanh_best.pth\n",
      "   Val Loss: 4.9388\n",
      "   Val Accuracy: 37.62%\n",
      "\n",
      "‚è≠Ô∏è  Skipping linux_32d-tanh - already trained!\n",
      "   Checkpoint: linux_32d-tanh_best.pth\n",
      "   Val Loss: 5.0973\n",
      "   Val Accuracy: 36.35%\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING LINUX KERNEL MODELS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create datasets\n",
    "linux_dataset = WordPredictionDataset(linux_X, linux_y)\n",
    "linux_train_size = int((1 - VAL_SPLIT) * len(linux_dataset))\n",
    "linux_val_size = len(linux_dataset) - linux_train_size\n",
    "linux_train_dataset, linux_val_dataset = random_split(linux_dataset, [linux_train_size, linux_val_size])\n",
    "\n",
    "linux_train_loader = DataLoader(linux_train_dataset, batch_size=BATCH_SIZE, shuffle=True, \n",
    "                                num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "linux_val_loader = DataLoader(linux_val_dataset, batch_size=BATCH_SIZE, shuffle=False, \n",
    "                              num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "\n",
    "for config in model_configs:\n",
    "    model_name = f\"linux_{config['name']}\"\n",
    "    \n",
    "    # Skip if already trained\n",
    "    if model_name in verified_models:\n",
    "        print(f\"\\n‚è≠Ô∏è  Skipping {model_name} - already trained!\")\n",
    "        print(f\"   Checkpoint: {verified_models[model_name]['path']}\")\n",
    "        print(f\"   Val Loss: {verified_models[model_name]['val_loss']:.4f}\")\n",
    "        print(f\"   Val Accuracy: {verified_models[model_name]['val_accuracy']:.2f}%\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\nüöÄ Starting training for {model_name}...\")\n",
    "    \n",
    "    # Create model\n",
    "    model = create_mlp_model(\n",
    "        vocab_size=linux_vocab_size,\n",
    "        embedding_dim=config['embedding_dim'],\n",
    "        hidden_size=HIDDEN_SIZE,\n",
    "        context_length=CONTEXT_LENGTH,\n",
    "        activation=config['activation']\n",
    "    ).to(device)\n",
    "    \n",
    "    print(f\"\\nüìä Model: {model_name}\")\n",
    "    print(f\"   Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    # Train model\n",
    "    model, train_losses, val_losses, val_accuracies = train_model(\n",
    "        model, linux_train_loader, linux_val_loader, NUM_EPOCHS, LEARNING_RATE, device, model_name\n",
    "    )\n",
    "    \n",
    "    # Plot training curves\n",
    "    plot_training_curves(train_losses, val_losses, val_accuracies, model_name)\n",
    "    \n",
    "    # Generate sample predictions\n",
    "    print(f\"\\nüéØ Sample Predictions for {model_name}:\")\n",
    "    predictions = generate_sample_predictions(model, linux_X, linux_idx_to_word, device, num_samples=5)\n",
    "    for i, pred in enumerate(predictions, 1):\n",
    "        print(f\"   {i}. Context: '{pred['context']}' ‚Üí Predicted: '{pred['predicted']}'\")\n",
    "    \n",
    "    # Save results\n",
    "    all_models[model_name] = {\n",
    "        'model': model,\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'val_accuracies': val_accuracies,\n",
    "        'config': config,\n",
    "        'final_val_loss': val_losses[-1],\n",
    "        'final_val_accuracy': val_accuracies[-1],\n",
    "        'vocab_size': linux_vocab_size,\n",
    "        'word_to_idx': linux_word_to_idx,\n",
    "        'idx_to_word': linux_idx_to_word,\n",
    "        'dataset_type': 'linux'\n",
    "    }\n",
    "    \n",
    "    # Incremental save after each model\n",
    "    with open('all_models_results.pkl', 'wb') as f:\n",
    "        pickle.dump(all_models, f)\n",
    "    print(f\"üíæ Saved checkpoint with {len(all_models)} models so far\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ {model_name} training complete!\")\n",
    "    print(f\"   Final Val Loss: {val_losses[-1]:.4f}\")\n",
    "    print(f\"   Final Val Accuracy: {val_accuracies[-1]:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbfdeab",
   "metadata": {},
   "source": [
    "## Final Save and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc6f43b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SAVING ALL MODELS AND RESULTS\n",
      "================================================================================\n",
      "‚úÖ All 8 models saved to 'all_models_results.pkl'\n",
      "\n",
      "================================================================================\n",
      "TRAINING SUMMARY - ALL 8 MODELS\n",
      "================================================================================\n",
      "\n",
      "Model                     Val Loss     Val Acc (%)  Parameters     \n",
      "----------------------------------------------------------------------\n",
      "wiki_64d-ReLU             7.3232       13.34            86,014,491\n",
      "wiki_32d-ReLU             7.4249       12.37            83,332,795\n",
      "wiki_64d-tanh             7.4166       13.23            86,014,491\n",
      "wiki_32d-tanh             7.5366       12.30            83,332,795\n",
      "linux_64d-ReLU            4.9966       35.33            99,299,202\n",
      "linux_32d-ReLU            5.0820       34.16            96,227,138\n",
      "linux_64d-tanh            4.9388       37.62            99,299,202\n",
      "linux_32d-tanh            5.0973       36.35            96,227,138\n",
      "\n",
      "‚ú® All training complete! Ready for embedding visualization and Streamlit deployment.\n"
     ]
    }
   ],
   "source": [
    "# First: Load metrics from checkpoints into all_models if missing\n",
    "for model_name in all_models.keys():\n",
    "    if all_models[model_name].get('final_val_loss') is None:\n",
    "        ckpt_path = f\"{model_name}_best.pth\"\n",
    "        if os.path.exists(ckpt_path):\n",
    "            try:\n",
    "                ckpt = torch.load(ckpt_path, map_location='cpu')\n",
    "                all_models[model_name]['final_val_loss'] = ckpt.get('val_loss')\n",
    "                all_models[model_name]['final_val_accuracy'] = ckpt.get('val_accuracy')\n",
    "            except:\n",
    "                pass  # Keep as None if checkpoint unreadable\n",
    "\n",
    "# Save all results with complete metrics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAVING ALL MODELS AND RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "with open('all_models_results.pkl', 'wb') as f:\n",
    "    pickle.dump(all_models, f)\n",
    "\n",
    "print(f\"‚úÖ All {len(all_models)} models saved to 'all_models_results.pkl'\")\n",
    "\n",
    "## Summary Report\n",
    "\n",
    "# Print summary table\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING SUMMARY - ALL 8 MODELS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n{'Model':<25} {'Val Loss':<12} {'Val Acc (%)':<12} {'Parameters':<15}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for model_name, results in all_models.items():\n",
    "    # Get metrics safely\n",
    "    val_loss = results.get('final_val_loss')\n",
    "    val_acc = results.get('final_val_accuracy')\n",
    "    \n",
    "    # Format strings\n",
    "    val_loss_str = f\"{val_loss:.4f}\" if val_loss is not None else \"N/A\"\n",
    "    val_acc_str = f\"{val_acc:.2f}\" if val_acc is not None else \"N/A\"\n",
    "    \n",
    "    # Get parameter count\n",
    "    if results.get('model') is not None:\n",
    "        params = sum(p.numel() for p in results['model'].parameters())\n",
    "    else:\n",
    "        # Reconstruct model temporarily to count params\n",
    "        if 'wiki' in model_name:\n",
    "            vocab_size = wiki_vocab_size\n",
    "        else:\n",
    "            vocab_size = linux_vocab_size\n",
    "        \n",
    "        embedding_dim = 64 if '64d' in model_name else 32\n",
    "        activation = 'tanh' if 'tanh' in model_name else 'relu'\n",
    "        \n",
    "        temp_model = create_mlp_model(vocab_size, embedding_dim, HIDDEN_SIZE, CONTEXT_LENGTH, activation)\n",
    "        params = sum(p.numel() for p in temp_model.parameters())\n",
    "        del temp_model\n",
    "    \n",
    "    print(f\"{model_name:<25} {val_loss_str:<12} {val_acc_str:<12} {params:>14,}\")\n",
    "\n",
    "print(\"\\n‚ú® All training complete! Ready for embedding visualization and Streamlit deployment.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
