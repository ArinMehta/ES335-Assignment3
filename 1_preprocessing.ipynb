{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77841f9f",
   "metadata": {},
   "source": [
    "# Part 1: Data Preprocessing and Vocabulary Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1409934e",
   "metadata": {},
   "source": [
    "## Installation Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "173c436e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (1.26.4)\n",
      "Requirement already satisfied: pandas in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (2.1.4)\n",
      "Requirement already satisfied: matplotlib in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (3.8.2)\n",
      "Requirement already satisfied: requests in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (2.32.5)\n",
      "Requirement already satisfied: tqdm in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (4.67.1)\n",
      "Requirement already satisfied: scikit-learn in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (1.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from requests) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from requests) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from requests) (2025.10.5)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from scikit-learn) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install numpy pandas matplotlib requests tqdm scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8007eb",
   "metadata": {},
   "source": [
    "## Dataset Download Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7b2c0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "def download_file(url, filename):\n",
    "    \"\"\"Download file with progress bar\"\"\"\n",
    "    if os.path.exists(filename):\n",
    "        print(f\"{filename} already exists. Skipping download.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Downloading {filename}...\")\n",
    "    response = requests.get(url, stream=True)\n",
    "    total_size = int(response.headers.get('content-length', 0))\n",
    "    \n",
    "    with open(filename, 'wb') as f, tqdm(\n",
    "        desc=filename,\n",
    "        total=total_size,\n",
    "        unit='iB',\n",
    "        unit_scale=True,\n",
    "        unit_divisor=1024,\n",
    "    ) as pbar:\n",
    "        for data in response.iter_content(chunk_size=1024):\n",
    "            size = f.write(data)\n",
    "            pbar.update(size)\n",
    "    print(f\"Downloaded {filename}\")\n",
    "\n",
    "# Download Wikipedia dataset (using smaller enwik8 for faster training)\n",
    "def download_wikipedia():\n",
    "    \"\"\"Download Wikipedia enwik8 dataset\"\"\"\n",
    "    url = \"http://mattmahoney.net/dc/enwik8.zip\"\n",
    "    download_file(url, \"enwik8.zip\")\n",
    "    \n",
    "    # Unzip\n",
    "    import zipfile\n",
    "    with zipfile.ZipFile(\"enwik8.zip\", 'r') as zip_ref:\n",
    "        zip_ref.extractall(\".\")\n",
    "    print(\"Wikipedia dataset extracted!\")\n",
    "\n",
    "# Download Linux Kernel Code\n",
    "def download_linux_kernel():\n",
    "    \"\"\"Download Linux kernel source code dataset\"\"\"\n",
    "    url = \"https://cs.stanford.edu/people/karpathy/char-rnn/linux_input.txt\"\n",
    "    download_file(url, \"linux_input.txt\")\n",
    "    print(\"Linux kernel dataset downloaded!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf74bff",
   "metadata": {},
   "source": [
    "## Execute Downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c8f3dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enwik8.zip already exists. Skipping download.\n",
      "Wikipedia dataset extracted!\n",
      "linux_input.txt already exists. Skipping download.\n",
      "Linux kernel dataset downloaded!\n"
     ]
    }
   ],
   "source": [
    "# Download both datasets\n",
    "download_wikipedia()\n",
    "download_linux_kernel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891f752a",
   "metadata": {},
   "source": [
    "## Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de813687",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_text_natural_language(text, max_chars=10000000):\n",
    "    \"\"\"\n",
    "    Preprocess natural language text (Wikipedia)\n",
    "    Remove special characters except full stop\n",
    "    \"\"\"\n",
    "    # Take only first max_chars to speed up processing\n",
    "    text = text[:max_chars]\n",
    "    \n",
    "    # Remove special characters except alphanumeric, space, and full stop\n",
    "    text = re.sub(r'[^a-zA-Z0-9 \\.]', '', text)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    text = re.sub(r' +', ' ', text)\n",
    "    text = re.sub(r'\\.+', '.', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def preprocess_code(text, max_chars=10000000):\n",
    "    \"\"\"\n",
    "    Preprocess code text (Linux Kernel)\n",
    "    Keep special characters but split by newlines\n",
    "    \"\"\"\n",
    "    # Take only first max_chars\n",
    "    text = text[:max_chars]\n",
    "    \n",
    "    # Convert to lowercase for consistency\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Replace tabs with spaces\n",
    "    text = text.replace('\\t', ' ')\n",
    "    \n",
    "    # Replace multiple spaces with single space\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def build_vocabulary(text, is_code=False):\n",
    "    \"\"\"\n",
    "    Build vocabulary from preprocessed text\n",
    "    Returns: word_to_idx, idx_to_word, word_counts\n",
    "    \"\"\"\n",
    "    if is_code:\n",
    "        # Split by newlines for code, treat each line as a sentence\n",
    "        lines = text.split('\\n')\n",
    "        # Split each line into words\n",
    "        words = []\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if line:  # Skip empty lines\n",
    "                words.extend(line.split())\n",
    "                words.append('<EOS>')  # End of statement marker\n",
    "    else:\n",
    "        # Split by periods for natural language\n",
    "        sentences = text.split('.')\n",
    "        words = []\n",
    "        for sentence in sentences:\n",
    "            sentence = sentence.strip()\n",
    "            if sentence:\n",
    "                words.extend(sentence.split())\n",
    "                words.append('<EOS>')  # End of sentence marker\n",
    "    \n",
    "    # Count word frequencies\n",
    "    word_counts = Counter(words)\n",
    "    \n",
    "    # Create vocabulary (keep all unique words)\n",
    "    unique_words = ['<PAD>', '<UNK>', '<EOS>'] + sorted(word_counts.keys())\n",
    "    \n",
    "    # Remove duplicates\n",
    "    unique_words = list(dict.fromkeys(unique_words))\n",
    "    \n",
    "    word_to_idx = {word: idx for idx, word in enumerate(unique_words)}\n",
    "    idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "    \n",
    "    return word_to_idx, idx_to_word, word_counts\n",
    "\n",
    "def create_training_data(text, word_to_idx, context_length=5, is_code=False):\n",
    "    \"\"\"\n",
    "    Create X, y pairs for training\n",
    "    X: context_length previous words\n",
    "    y: next word to predict\n",
    "    \"\"\"\n",
    "    if is_code:\n",
    "        lines = text.split('\\n')\n",
    "        words = []\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                words.extend(line.split())\n",
    "                words.append('<EOS>')\n",
    "    else:\n",
    "        sentences = text.split('.')\n",
    "        words = []\n",
    "        for sentence in sentences:\n",
    "            sentence = sentence.strip()\n",
    "            if sentence:\n",
    "                words.extend(sentence.split())\n",
    "                words.append('<EOS>')\n",
    "    \n",
    "    # Convert words to indices\n",
    "    word_indices = [word_to_idx.get(word, word_to_idx['<UNK>']) for word in words]\n",
    "    \n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    # Create context windows\n",
    "    for i in range(len(word_indices) - context_length):\n",
    "        context = word_indices[i:i + context_length]\n",
    "        target = word_indices[i + context_length]\n",
    "        X.append(context)\n",
    "        y.append(target)\n",
    "    \n",
    "    return np.array(X, dtype=np.int64), np.array(y, dtype=np.int64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d52225",
   "metadata": {},
   "source": [
    "## Process Wikipedia Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "352d5975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PROCESSING WIKIPEDIA DATASET (Category I: Natural Language)\n",
      "================================================================================\n",
      "\n",
      "Preprocessing text...\n",
      "Building vocabulary...\n",
      "\n",
      "üìä WIKIPEDIA DATASET STATISTICS:\n",
      "   Vocabulary size: 78683\n",
      "   Total words in corpus: 674778\n",
      "\n",
      "   üîù 10 Most Frequent Words:\n",
      "      1. 'the': 38601 occurrences\n",
      "      2. 'of': 24013 occurrences\n",
      "      3. 'and': 16622 occurrences\n",
      "      4. 'in': 14576 occurrences\n",
      "      5. 'to': 11893 occurrences\n",
      "      6. 'a': 11694 occurrences\n",
      "      7. 'is': 6792 occurrences\n",
      "      8. 'as': 5091 occurrences\n",
      "      9. 'for': 4375 occurrences\n",
      "      10. 'that': 4347 occurrences\n",
      "\n",
      "   üîª 10 Least Frequent Words:\n",
      "      1. 'id1116id': 1 occurrences\n",
      "      2. 'id15899620id': 1 occurrences\n",
      "      3. 'timestamp20020909t020201ztimestamp': 1 occurrences\n",
      "      4. 'commentmerge': 1 occurrences\n",
      "      5. 'samoatext': 1 occurrences\n",
      "      6. 'titleaustraliageographytitle': 1 occurrences\n",
      "      7. 'id1117id': 1 occurrences\n",
      "      8. 'id15899621id': 1 occurrences\n",
      "      9. 'timestamp20020804t102336ztimestamp': 1 occurrences\n",
      "      10. 'australiacom': 1 occurrences\n",
      "\n",
      "\n",
      "Creating training data (context_length=5)...\n",
      "   Training samples created: 674773\n",
      "   X shape: (674773, 5)\n",
      "   y shape: (674773,)\n",
      "\n",
      "   Example training pair:\n",
      "   Context (X): ['id32899315id', 'timestamp20051227t184647ztimestamp', 'contributor', 'usernamejsmethersusername', 'id614213id']\n",
      "   Target (y):  contributor\n",
      "\n",
      "‚úÖ Wikipedia data saved to 'wikipedia_processed.pkl'\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"PROCESSING WIKIPEDIA DATASET (Category I: Natural Language)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Read the file\n",
    "with open('enwik8', 'rb') as f:\n",
    "    raw_text = f.read().decode('utf-8', errors='ignore')\n",
    "\n",
    "# Preprocess\n",
    "print(\"\\nPreprocessing text...\")\n",
    "wiki_text = preprocess_text_natural_language(raw_text, max_chars=5000000)  # Use 5MB for faster processing\n",
    "\n",
    "# Build vocabulary\n",
    "print(\"Building vocabulary...\")\n",
    "wiki_word_to_idx, wiki_idx_to_word, wiki_word_counts = build_vocabulary(wiki_text, is_code=False)\n",
    "\n",
    "# Print statistics\n",
    "print(f\"\\nüìä WIKIPEDIA DATASET STATISTICS:\")\n",
    "print(f\"   Vocabulary size: {len(wiki_word_to_idx)}\")\n",
    "print(f\"   Total words in corpus: {sum(wiki_word_counts.values())}\")\n",
    "\n",
    "# Get 10 most frequent words (excluding special tokens)\n",
    "most_common = wiki_word_counts.most_common(13)  # Get extra to exclude special tokens\n",
    "most_common_filtered = [w for w in most_common if w[0] not in ['<PAD>', '<UNK>', '<EOS>']][:10]\n",
    "print(f\"\\n   üîù 10 Most Frequent Words:\")\n",
    "for i, (word, count) in enumerate(most_common_filtered, 1):\n",
    "    print(f\"      {i}. '{word}': {count} occurrences\")\n",
    "\n",
    "# Get 10 least frequent words\n",
    "least_common = wiki_word_counts.most_common()[-10:]\n",
    "print(f\"\\n   üîª 10 Least Frequent Words:\")\n",
    "for i, (word, count) in enumerate(least_common, 1):\n",
    "    print(f\"      {i}. '{word}': {count} occurrences\")\n",
    "\n",
    "# Create training data with context length 5\n",
    "print(\"\\n\\nCreating training data (context_length=5)...\")\n",
    "wiki_X, wiki_y = create_training_data(wiki_text, wiki_word_to_idx, context_length=5, is_code=False)\n",
    "\n",
    "print(f\"   Training samples created: {len(wiki_X)}\")\n",
    "print(f\"   X shape: {wiki_X.shape}\")\n",
    "print(f\"   y shape: {wiki_y.shape}\")\n",
    "\n",
    "# Show example (FIXED)\n",
    "print(f\"\\n   Example training pair:\")\n",
    "context_words = [wiki_idx_to_word[int(idx)] for idx in wiki_X[100].tolist()]\n",
    "target_word = wiki_idx_to_word[int(wiki_y[100])]\n",
    "print(f\"   Context (X): {context_words}\")\n",
    "print(f\"   Target (y):  {target_word}\")\n",
    "\n",
    "# Save processed data\n",
    "import pickle\n",
    "\n",
    "wiki_data = {\n",
    "    'X': wiki_X,\n",
    "    'y': wiki_y,\n",
    "    'word_to_idx': wiki_word_to_idx,\n",
    "    'idx_to_word': wiki_idx_to_word,\n",
    "    'word_counts': wiki_word_counts,\n",
    "    'vocab_size': len(wiki_word_to_idx),\n",
    "    'text_sample': wiki_text[:10000]  # Save a sample for reference\n",
    "}\n",
    "\n",
    "with open('wikipedia_processed.pkl', 'wb') as f:\n",
    "    pickle.dump(wiki_data, f)\n",
    "\n",
    "print(\"\\n‚úÖ Wikipedia data saved to 'wikipedia_processed.pkl'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262fb874",
   "metadata": {},
   "source": [
    "## Process Linux Kernel Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc0f2668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PROCESSING LINUX KERNEL DATASET (Category II: Structured Code)\n",
      "================================================================================\n",
      "\n",
      "Preprocessing code...\n",
      "Building vocabulary...\n",
      "\n",
      "üìä LINUX KERNEL DATASET STATISTICS:\n",
      "   Vocabulary size: 90882\n",
      "   Total words in corpus: 779608\n",
      "\n",
      "   üîù 10 Most Frequent Words:\n",
      "      1. '<EOS>': 165787 occurrences\n",
      "      2. '*': 28256 occurrences\n",
      "      3. '=': 22173 occurrences\n",
      "      4. 'if': 15275 occurrences\n",
      "      5. 'the': 14721 occurrences\n",
      "      6. '{': 14702 occurrences\n",
      "      7. '}': 13567 occurrences\n",
      "      8. '*/': 10962 occurrences\n",
      "      9. '/*': 9882 occurrences\n",
      "      10. 'struct': 8870 occurrences\n",
      "\n",
      "   üîª 10 Least Frequent Words:\n",
      "      1. 'buffer_b;': 1 occurrences\n",
      "      2. 'out_dec:': 1 occurrences\n",
      "      3. 'atomic_dec(&cpu_buffer_a->record_disabled);': 1 occurrences\n",
      "      4. 'atomic_dec(&cpu_buffer_b->record_disabled);': 1 occurrences\n",
      "      5. 'export_symbol_gpl(ring_buffer_swap_cpu);': 1 occurrences\n",
      "      6. 'ring_buffer_alloc_read_page': 1 occurrences\n",
      "      7. 'ring_buffer_read_page.': 1 occurrences\n",
      "      8. 'ring_buffer_read_page,': 1 occurrences\n",
      "      9. '*ring_buffer_alloc_read_page(struct': 1 occurrences\n",
      "      10. '*pag': 1 occurrences\n",
      "\n",
      "\n",
      "Creating training data (context_length=5)...\n",
      "   Training samples created: 779603\n",
      "   X shape: (779603, 5)\n",
      "   y shape: (779603,)\n",
      "\n",
      "   Example training pair:\n",
      "   Context (X): ['*', '<EOS>', '*', 'commence', 'probing']\n",
      "   Target (y):  for\n",
      "\n",
      "‚úÖ Linux kernel data saved to 'linux_processed.pkl'\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PROCESSING LINUX KERNEL DATASET (Category II: Structured Code)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Read the file\n",
    "with open('linux_input.txt', 'r', encoding='utf-8', errors='ignore') as f:\n",
    "    raw_code = f.read()\n",
    "\n",
    "# Preprocess\n",
    "print(\"\\nPreprocessing code...\")\n",
    "linux_text = preprocess_code(raw_code, max_chars=5000000)  # Use 5MB for faster processing\n",
    "\n",
    "# Build vocabulary\n",
    "print(\"Building vocabulary...\")\n",
    "linux_word_to_idx, linux_idx_to_word, linux_word_counts = build_vocabulary(linux_text, is_code=True)\n",
    "\n",
    "# Print statistics\n",
    "print(f\"\\nüìä LINUX KERNEL DATASET STATISTICS:\")\n",
    "print(f\"   Vocabulary size: {len(linux_word_to_idx)}\")\n",
    "print(f\"   Total words in corpus: {sum(linux_word_counts.values())}\")\n",
    "\n",
    "# Get 10 most frequent words\n",
    "most_common = linux_word_counts.most_common(13)\n",
    "most_common_filtered = [w for w in most_common if w not in ['<PAD>', '<UNK>', '<EOS>']][:10]\n",
    "print(f\"\\n   üîù 10 Most Frequent Words:\")\n",
    "for i, (word, count) in enumerate(most_common_filtered, 1):\n",
    "    print(f\"      {i}. '{word}': {count} occurrences\")\n",
    "\n",
    "# Get 10 least frequent words\n",
    "least_common = linux_word_counts.most_common()[-10:]\n",
    "print(f\"\\n   üîª 10 Least Frequent Words:\")\n",
    "for i, (word, count) in enumerate(least_common, 1):\n",
    "    print(f\"      {i}. '{word}': {count} occurrences\")\n",
    "\n",
    "# Create training data\n",
    "print(\"\\n\\nCreating training data (context_length=5)...\")\n",
    "linux_X, linux_y = create_training_data(linux_text, linux_word_to_idx, context_length=5, is_code=True)\n",
    "\n",
    "print(f\"   Training samples created: {len(linux_X)}\")\n",
    "print(f\"   X shape: {linux_X.shape}\")\n",
    "print(f\"   y shape: {linux_y.shape}\")\n",
    "\n",
    "# Show example\n",
    "print(f\"\\n   Example training pair:\")\n",
    "context_words = [linux_idx_to_word[int(idx)] for idx in linux_X[100].tolist()]\n",
    "target_word = linux_idx_to_word[int(linux_y[100])]\n",
    "print(f\"   Context (X): {context_words}\")\n",
    "print(f\"   Target (y):  {target_word}\")\n",
    "\n",
    "\n",
    "# Save processed data\n",
    "linux_data = {\n",
    "    'X': linux_X,\n",
    "    'y': linux_y,\n",
    "    'word_to_idx': linux_word_to_idx,\n",
    "    'idx_to_word': linux_idx_to_word,\n",
    "    'word_counts': linux_word_counts,\n",
    "    'vocab_size': len(linux_word_to_idx),\n",
    "    'text_sample': linux_text[:10000]\n",
    "}\n",
    "\n",
    "with open('linux_processed.pkl', 'wb') as f:\n",
    "    pickle.dump(linux_data, f)\n",
    "\n",
    "print(\"\\n‚úÖ Linux kernel data saved to 'linux_processed.pkl'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8526a6fa",
   "metadata": {},
   "source": [
    "## Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9adc676f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DATA PREPROCESSING COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "üì¶ Files created:\n",
      "   1. wikipedia_processed.pkl\n",
      "   2. linux_processed.pkl\n",
      "\n",
      "‚ú® Ready for model training!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DATA PREPROCESSING COMPLETE!\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nüì¶ Files created:\")\n",
    "print(\"   1. wikipedia_processed.pkl\")\n",
    "print(\"   2. linux_processed.pkl\")\n",
    "print(\"\\n‚ú® Ready for model training!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
